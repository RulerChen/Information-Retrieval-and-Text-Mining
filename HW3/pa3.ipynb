{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import math\n",
    "import csv\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    tokens = document.split() \n",
    "    lowercase_tokens = [token.lower() for token in tokens] \n",
    "\n",
    "    stopwords = set()\n",
    "    with open('./stopwords.txt', 'r', encoding='utf-8') as stopword_file:\n",
    "        stopwords = set(stopword_file.read().splitlines())\n",
    "    filtered_tokens = [token for token in lowercase_tokens if token not in stopwords]\n",
    "\n",
    "    filtered_tokens_without_endings = [re.sub(r'[,.!?\"@()%`\\':;{}$&*-]+', '', token) for token in filtered_tokens]\n",
    "    filtered_tokens_without_endings = [token for token in filtered_tokens_without_endings if token != '']\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens_without_endings]\n",
    "    stemmed_tokens = [token for token in stemmed_tokens if token not in stopwords]\n",
    "\n",
    "    words = [token for token in stemmed_tokens if not token.isdigit() and len(token) > 1]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word_frequencies(training_data):\n",
    "    term_category_freq = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    term_present_num = defaultdict(lambda: 0)\n",
    "\n",
    "    for category, doc_ids in training_data.items():\n",
    "        for doc_id in doc_ids:\n",
    "            with open(f'./data/{doc_id}.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "                document = file.read()\n",
    "                tokens = preprocessing(document)\n",
    "                tokens = list(set(tokens))\n",
    "                for term in tokens:\n",
    "                    term_category_freq[category][term] += 1\n",
    "                    term_present_num[term] += 1\n",
    "\n",
    "    return term_category_freq, term_present_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood_ratios(term_category_freq, term_present_num):\n",
    "    total_doc_num = 195\n",
    "    category_doc_num = 15\n",
    "\n",
    "    word_freq_table = defaultdict(lambda: defaultdict(lambda: [0,0,0,0])) # n11, n01, n10, n00\n",
    "    \n",
    "    for category, category_freq in term_category_freq.items():\n",
    "        for term, freq in category_freq.items():\n",
    "            word_freq_table[category][term][0] = freq\n",
    "            word_freq_table[category][term][1] = term_present_num[term] - freq\n",
    "            word_freq_table[category][term][2] = category_doc_num - freq\n",
    "            word_freq_table[category][term][3] = total_doc_num - term_present_num[term] - category_doc_num + freq\n",
    "    \n",
    "    likelihood_ratio_table = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for category, category_freq in word_freq_table.items():\n",
    "        for term, freq in category_freq.items():\n",
    "            n11, n01, n10, n00 = freq\n",
    "            N = n11 + n10 + n01 + n00\n",
    "            score = (((n11 + n01) / N) ** n11) * ((1 - ((n11 + n01) / N)) ** n10) * ((( n11 + n01) / N) ** n01) * ((1 - ((n11 + n01) / N)) ** n00)\n",
    "            score /= ((n11 / (n11 + n10)) ** n11) * ((1 - (n11 / (n11 + n10))) ** n10) * ((n01 / (n01 + n00)) ** n01) * ((1 - (n01 / (n01 + n00))) ** n00)\n",
    "            score = -2 * math.log(score)\n",
    "            likelihood_ratio_table[category][term] = score\n",
    "\n",
    "    top_words_class = {}\n",
    "    select_feature_num = 30\n",
    "\n",
    "    for category, term_score in likelihood_ratio_table.items():\n",
    "        word = []\n",
    "        for term, score in term_score.items():\n",
    "            word.append([term, score])\n",
    "        top_words_class[category] = word\n",
    "    for category, words in top_words_class.items():\n",
    "        words.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_words_class[category] = words[:select_feature_num]\n",
    "\n",
    "    features = []\n",
    "    for category, words in top_words_class.items():\n",
    "        for i in range(len(words)):\n",
    "            if words[i][0] not in features:\n",
    "                features.append(words[i][0])\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(training_data, features):\n",
    "    total_docs = 195\n",
    "\n",
    "    prior_probabilities = {category: len(docs) / total_docs for category, docs in training_data.items()}\n",
    "    \n",
    "    conditional_probabilities = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for category, docs in training_data.items():\n",
    "        category_word_freq = defaultdict(lambda: 0)\n",
    "        total_words = 0\n",
    "        for doc in docs:\n",
    "            with open(f'./data/{doc}.txt', \"r\", encoding=\"utf-8\") as file:\n",
    "                document = file.read()\n",
    "                tokens = preprocessing(document)\n",
    "                for word in tokens:\n",
    "                    if word in features:\n",
    "                        category_word_freq[word] += 1\n",
    "                        total_words += 1\n",
    "                    else:\n",
    "                        total_words += 1\n",
    "        \n",
    "        for word in features:\n",
    "            conditional_probabilities[category][word] = (category_word_freq[word] + 1) / total_words + len(features)\n",
    "\n",
    "    return prior_probabilities, conditional_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(document, prior_probabilities, conditional_probabilities, features):\n",
    "    tokens = preprocessing(document)  \n",
    "    probabilities = {}\n",
    "    for category in prior_probabilities:\n",
    "        log_prob = math.log(prior_probabilities[category])\n",
    "        for word in tokens:\n",
    "            if word in features:\n",
    "                log_prob += math.log(conditional_probabilities[category][word])\n",
    "        probabilities[category] = log_prob\n",
    "    predicted_category = max(probabilities, key=probabilities.get)\n",
    "\n",
    "    return predicted_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = {}\n",
    "with open(f\"./training.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        category = int(parts[0])\n",
    "        document_ids = [int(id) for id in parts[1:]]\n",
    "        training_data[category] = document_ids\n",
    "\n",
    "term_category_freq, term_present_num = calculate_word_frequencies(training_data)\n",
    "features = calculate_likelihood_ratios(term_category_freq, term_present_num)\n",
    "prior_probabilities, conditional_probabilities = train_classifier(training_data, features)\n",
    "\n",
    "predictions = []\n",
    "for doc_id in range(1, 1096):\n",
    "    if all(doc_id not in docs for docs in training_data.values()):\n",
    "        with open(f'./data/{doc_id}.txt', 'r', encoding='utf-8') as file:\n",
    "            document = file.read()\n",
    "            predicted_class = predict(document, prior_probabilities, conditional_probabilities, features)\n",
    "            predictions.append((doc_id, predicted_class))\n",
    "\n",
    "with open('./predict.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Id', 'Value'])\n",
    "    for doc_id, pred_class in predictions:\n",
    "        writer.writerow([doc_id, pred_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
